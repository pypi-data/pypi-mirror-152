{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "fbNXzKJY9W_8",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJqgVMV09W_-"
   },
   "source": [
    "To use MICE, first you need to define a function with the gradient that you want to approximate.\n",
    "For example, here we have a two-dimensional quadratic function,\n",
    "\n",
    "$$\n",
    "f(\\boldsymbol{\\xi}, \\theta) = \\frac{1}{2} \\boldsymbol{\\xi} \\cdot \\boldsymbol{H}(\\theta) \\boldsymbol{\\xi}\n",
    "- \\boldsymbol{b} \\cdot \\boldsymbol{\\xi}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\boldsymbol{\\xi} \\quad & \\textrm{ is a vector containing the optimization variables, } \\\\\n",
    "    \\theta \\quad & \\textrm{ is a random variable, } \\\\\n",
    "    f(\\boldsymbol{\\xi}, \\theta) \\quad & \\textrm{ is a function whose expectation wrt } \\theta \\textrm{ is to be minimized, } \\\\\n",
    "    \\boldsymbol{H}(\\theta) \\quad & \\textrm{ is the Hessian matrix, } \\\\\n",
    "    \\boldsymbol{b} \\quad & \\textrm{is a vector of the same dimension as } \\boldsymbol{\\xi}.\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "Here, we have a Hessian that is a convex combination of the identity matrix and of an arbitrary matrix:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{H}(\\theta) =\n",
    "\\boldsymbol{I}_2(1 -\\theta) +\n",
    "\\begin{bmatrix}\n",
    "\\kappa & \\kappa-1 \\\\\n",
    "        \\kappa-1 & \\kappa\n",
    "\\end{bmatrix}\n",
    "\\theta,\n",
    "$$\n",
    "\n",
    "noting that $\\theta$ is a random variable such that $\\theta \\sim \\mathcal{U}[0, 1]$\n",
    "and $\\kappa$ is the desired conditioning number of the expected value of the Hessian.\n",
    "Also, $\\boldsymbol{b}$ is a 2-dimensional vector with ones.\n",
    "\n",
    "To use the stochastic gradient method, we need the gradient of $f$ wrt $\\boldsymbol{\\xi}$:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{\\xi}} f(\\boldsymbol{\\xi}, \\theta) = \\boldsymbol{H}(\\theta) \\boldsymbol{\\xi} - \\boldsymbol{b}.\n",
    "$$\n",
    "\n",
    "To use MICE to estimate $\\nabla_{\\boldsymbol{\\xi}} f$, we need to import the MICE class and NumPy to assist us in\n",
    "defining our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oyf1IPOy9XAA",
    "outputId": "4eb99b6a-d70f-4d46-e43c-7f896cc6e30c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mice in /home/andre/.local/lib/python3.8/site-packages (0.1.5)\n",
      "Requirement already satisfied: pandas>=1.1 in /home/andre/.local/lib/python3.8/site-packages (from mice) (1.3.3)\n",
      "Requirement already satisfied: matplotlib>=3.2.0 in /home/andre/.local/lib/python3.8/site-packages (from mice) (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.19 in /home/andre/.local/lib/python3.8/site-packages (from mice) (1.20.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.2.0->mice) (2.7.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/andre/.local/lib/python3.8/site-packages (from matplotlib>=3.2.0->mice) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/andre/.local/lib/python3.8/site-packages (from matplotlib>=3.2.0->mice) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/andre/.local/lib/python3.8/site-packages (from matplotlib>=3.2.0->mice) (1.2.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from cycler>=0.10->matplotlib>=3.2.0->mice) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas>=1.1->mice) (2019.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mice\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mice import MICE, plot_mice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "0VVXplSp9XAC",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Then, we need to define $\\nabla_{\\boldsymbol{\\xi}} f$ as a function of $\\boldsymbol{\\xi}$ and an array representing a sample of $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "krCvP3Zc9XAC"
   },
   "outputs": [],
   "source": [
    "kappa = 100\n",
    "H_aux = np.array([[kappa, kappa-1], [kappa-1, kappa]])\n",
    "b = np.ones(2)\n",
    "\n",
    "\n",
    "def dobjf(x, thetas):\n",
    "    gradients = []\n",
    "    for theta in thetas:\n",
    "        H = np.eye(2) * (1 - theta) + H_aux * theta\n",
    "        gradients.append(H @ x.T - b)\n",
    "    return np.vstack(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q__y_Ynp9XAD"
   },
   "source": [
    "As for $\\theta$, we can pass to MICE either a sampler function that takes the desired sample size as input and returns the sample\n",
    "or a list/array containing the data.\n",
    "Here, we will define a function that returns the uniform sample between 0 and 1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CdHcRIwv9XAE"
   },
   "outputs": [],
   "source": [
    "def sampler(n):\n",
    "    return np.random.uniform(0, 1, int(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzrlqACd9XAF"
   },
   "source": [
    "Now, let's create an instance of MICE to solve this optimization problem with tolerance to statistical error of $\\epsilon=0.7$,\n",
    "maximum cost of $10,000$ evaluations of $\\nabla_{\\boldsymbol{\\xi}} f$, and a minimum batch size of $5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "neQcr2Kp9XAG"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-e042ce1993e5>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-e042ce1993e5>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    stop_crit_norm=1)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df = MICE(dobjf,\n",
    "          sampler=sampler,\n",
    "          eps=.7,\n",
    "          max_cost=1e4,\n",
    "          min_batch=5\n",
    "          stop_crit_norm=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fL6k8kcg9XAH"
   },
   "source": [
    "To perform optimization, we need to set a starting point and a step size. \n",
    "Here, we know both the $L$-smoothness and the $\\mu$-convexity parameters of the problem, thus we can set the step size optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOzTl4WS9XAI"
   },
   "outputs": [],
   "source": [
    "x = np.array([20., 50.])\n",
    "L = kappa\n",
    "mu = 1\n",
    "step_size = 2 / (L + mu) / (1 + df.eps ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFXwv_Ez9XAI"
   },
   "source": [
    "and, finally, we iterate until MICE's cost is reached, in which case df.terminate returns True,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLfbFvi_9XAJ"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    grad = df.evaluate(x)\n",
    "    if df.terminate:\n",
    "        break\n",
    "    x = x - step_size * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mM_n5leA9XAJ"
   },
   "source": [
    "Finally, let's plot the convergence of the norm of MICE's gradient estimates from the log DataFrame using the built-in plot_mice function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "gnRLzwsH9XAK",
    "outputId": "4fb2a412-3986-4bb7-d02e-0ca4b273715c"
   },
   "outputs": [],
   "source": [
    "log = df.get_log()\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ax = plot_mice(log, ax, 'iteration', 'grad_norm', style='semilogy')\n",
    "ax.axhline(df.stop_crit_norm, ls='--', c='k', label='Gradient norm tolerance')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Norm of estimate')\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "getting_started.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
