Metadata-Version: 2.1
Name: onnxruntime-openvino
Version: 1.11.0
Summary: ONNX Runtime is a runtime accelerator for Machine Learning models
Home-page: https://onnxruntime.ai
Author: Microsoft Corporation
Author-email: onnxruntime@microsoft.com
License: MIT License
Download-URL: https://github.com/microsoft/onnxruntime/tags
Keywords: onnx machine learning
Platform: UNKNOWN
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Operating System :: POSIX :: Linux
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Scientific/Engineering :: Mathematics
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development
Classifier: Topic :: Software Development :: Libraries
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Operating System :: Microsoft :: Windows
Requires-Dist: numpy (>=1.19.5)
Requires-Dist: protobuf
Requires-Dist: flatbuffers

OpenVINOâ„¢ Execution Provider for ONNX Runtime
===============================================

`OpenVINOâ„¢ Execution Provider for ONNX Runtime <https://onnxruntime.ai/docs/execution-providers/OpenVINO-ExecutionProvider.html>`_ is a product designed for ONNX Runtime developers who want to get started with OpenVINOâ„¢ in their inferencing applications. This product delivers  `OpenVINOâ„¢ <https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html>`_ inline optimizations which enhance inferencing performance with minimal code modifications. 

OpenVINOâ„¢ Execution Provider for ONNX Runtime accelerates inference across many  `AI models <https://github.com/onnx/models>`_ on a variety of IntelÂ® hardware such as:
 - IntelÂ® CPUs
 - IntelÂ® integrated GPUs
 - IntelÂ® Movidiusâ„¢ Vision Processing Units - referred to as VPU.


Installation
------------

Requirements
^^^^^^^^^^^^

- Ubuntu 18.04, 20.04, RHEL(CPU only) or Windows 10 - 64 bit
- Python 3.7, 3.8 or 3.9

This package supports:
 - IntelÂ® CPUs
 - IntelÂ® integrated GPUs
 - IntelÂ® Movidiusâ„¢ Vision Processing Units (VPUs).

Please Note for VAD-M use Docker installation / Build from Source for Linux. 

``pip3 install onnxruntime-openvino==1.11.0``

Windows release supports only Python 3.9. Please install OpenVINOâ„¢ PyPi Package separately for Windows. 
For installation instructions on Windows please refer to  `OpenVINOâ„¢ Execution Provider for ONNX Runtime for Windows <https://github.com/intel/onnxruntime/releases/>`_.

This **OpenVINOâ„¢ Execution Provider for ONNX Runtime** Linux Wheels comes with pre-built libraries of OpenVINOâ„¢ version 2022.1.0 meaning you do not have to install OpenVINOâ„¢ separately. CXX11_ABI flag for pre built OpenVINOâ„¢ libraries is 0.

For more details on build and installation please refer to `Build <https://onnxruntime.ai/docs/build/eps.html#openvino>`_.

Usage
^^^^^

By default, IntelÂ® CPU is used to run inference. However, you can change the default option to either IntelÂ® integrated GPU or IntelÂ® VPU for AI inferencing. Invoke the following function to change the hardware on which inferencing is done.

For more API calls and environment variables, see  `Usage <https://onnxruntime.ai/docs/execution-providers/OpenVINO-ExecutionProvider.html#configuration-options>`_.

Samples
^^^^^^^^

To see what you can do with **OpenVINOâ„¢ Execution Provider for ONNX Runtime**, explore the demos located in the  `Examples <https://github.com/microsoft/onnxruntime-inference-examples/tree/main/python/OpenVINO_EP>`_.

Docker Support
^^^^^^^^^^^^^^

The latest OpenVINOâ„¢ EP docker image can be downloaded from DockerHub. 
For more details see  `Docker ReadMe <https://hub.docker.com/r/openvino/onnxruntime_ep_ubuntu18>`_.


Prebuilt Images
^^^^^^^^^^^^^^^^

- Please find prebuilt docker images for IntelÂ® CPU and IntelÂ® iGPU on OpenVINOâ„¢ Execution Provider `Release Page <https://github.com/intel/onnxruntime/releases/>`_. 

License
^^^^^^^^

**OpenVINOâ„¢ Execution Provider for ONNX Runtime** is licensed under `MIT <https://github.com/microsoft/onnxruntime/blob/master/LICENSE>`_.
By contributing to the project, you agree to the license and copyright terms therein
and release your contribution under these terms.  

Support
^^^^^^^^

Please submit your questions, feature requests and bug reports via   `GitHub Issues <https://github.com/microsoft/onnxruntime/issues>`_.

How to Contribute
^^^^^^^^^^^^^^^^^^

We welcome community contributions to **OpenVINOâ„¢ Execution Provider for ONNX Runtime**. If you have an idea for improvement:

* Share your proposal via  `GitHub Issues <https://github.com/microsoft/onnxruntime/issues>`_.
* Submit a  `Pull Request <https://github.com/microsoft/onnxruntime/pulls>`_.





