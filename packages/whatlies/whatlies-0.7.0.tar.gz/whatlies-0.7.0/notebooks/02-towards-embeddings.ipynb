{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards Sets of Embeddings \n",
    "\n",
    "The previous example was cute but usually we won't have embeddings that are mere 2d vectors. I want to understand \"what lies\" in general word embeddings. \n",
    "\n",
    "So, what can we do? We'll briefly talk about 2d embeddings in order to understand something about plotting higher dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 6)\n",
    "\n",
    "from whatlies import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imaginary Embeddings\n",
    "\n",
    "Let's take three embeddings as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man   = Embedding(\"man\", [0.5, 0.15])\n",
    "woman = Embedding(\"woman\", [0.35, 0.2])\n",
    "king  = Embedding(\"king\", [0.2, 0.2])\n",
    "\n",
    "man.plot(kind=\"arrow\", color=\"blue\")\n",
    "woman.plot(kind=\"arrow\", color=\"red\")\n",
    "king.plot(kind=\"arrow\", color=\"green\")\n",
    "\n",
    "plt.xlim(0, 0.5)\n",
    "plt.ylim(0, 0.5)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Unto Tokens\n",
    "\n",
    "In the previous notebook we demonstrated how to map \"away\" from vectors. But we can also map \"unto\" vectors. For this we introduce the `>>` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man.plot(kind=\"arrow\", color=\"blue\")\n",
    "woman.plot(kind=\"arrow\", color=\"red\")\n",
    "(woman >> man).plot(kind=\"arrow\", color=\"red\")\n",
    "(woman >> king).plot(kind=\"arrow\", color=\"red\")\n",
    "king.plot(kind=\"arrow\", color=\"green\")\n",
    "\n",
    "plt.xlim(0, 0.5)\n",
    "plt.ylim(0, 0.5)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the Mapping\n",
    "\n",
    "Note that the `woman` vector in our embedding maps partially unto `man` and overshoots a bit on `king`. We can quantify this by measuring what percentage of the vector is covered. This factor can be retreived by using the `>` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman > king, woman > man"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting \n",
    "\n",
    "The interesting thing here is that we can plot one token on the axis of another one. Our embeddings here were of a higher dimension then we can still perform the `>` operation. No matter how large the embedding, we could've said `woman` spans 1.375 of `king` and 0.752 of `man`. Given `king` as the x-axis and `man` as the y-axis, we can map the token of `man` to a 2d representation (1.375, 0.752) which is easy to plot.\n",
    "\n",
    "This is an interesting way of thinking about it. We can plot high dimensional vectors in 2d as long as we can plot it along two axes. An axis could be a vector of a token, or a token that has had operations on it. \n",
    "\n",
    "Note that this `>` mapping can also cause negative values. The example below demonstrates this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = Embedding(\"foo\", [-0.2, -0.2])\n",
    "\n",
    "foo.plot(kind=\"arrow\", color=\"pink\")\n",
    "woman.plot(kind=\"arrow\", color=\"red\")\n",
    "king.plot(kind=\"arrow\", color=\"green\")\n",
    "(foo >> woman).plot(kind=\"arrow\", color=\"red\", show_ops=True)\n",
    "\n",
    "plt.xlim(-.3, 0.4)\n",
    "plt.ylim(-.3, 0.4)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo > woman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Dimensions \n",
    "\n",
    "Let's confirm this idea by using some spaCy word-vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"cat\", \"dog\", \"fish\", \"kitten\", \"man\", \"woman\", \"king\", \"queen\", \"doctor\", \"nurse\"]\n",
    "tokens = {t.text: Embedding(t.text, t.vector) for t in nlp.pipe(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = tokens['man']\n",
    "y_axis = tokens['woman']\n",
    "for name, t in tokens.items():\n",
    "    t.plot(x_axis=x_axis, y_axis=y_axis).plot(kind=\"text\", x_axis=x_axis, y_axis=y_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting thing is that we can also perform operations on these words before plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "royalty = tokens['king'] - tokens['queen']\n",
    "gender = tokens['man'] - tokens['woman']\n",
    "for n, t in tokens.items():\n",
    "    (t\n",
    "      .plot(x_axis=royalty, y_axis=gender)\n",
    "      .plot(kind=\"text\", x_axis=royalty, y_axis=gender))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \n",
    "         \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\"]\n",
    "tokens = {t.text: Embedding(t.text, t.vector) for t in nlp.pipe(words)}\n",
    "x_axis = tokens['man']\n",
    "y_axis = tokens['woman']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "for name, t in tokens.items():\n",
    "    t.plot(x_axis=x_axis, y_axis=y_axis).plot(kind=\"text\", x_axis=x_axis, y_axis=y_axis)\n",
    "    plt.title('token')\n",
    "\n",
    "plt.subplot(122)\n",
    "space_to_filter = tokens['king'] - tokens['queen']\n",
    "for name, t in tokens.items():\n",
    "    ((t | space_to_filter)\n",
    "     .plot(x_axis=x_axis | space_to_filter, y_axis=y_axis | space_to_filter)\n",
    "     .plot(kind=\"text\", x_axis=x_axis | space_to_filter, y_axis=y_axis | space_to_filter))\n",
    "    plt.title('token | (king - queen)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets of Tokens \n",
    "\n",
    "These plots are cool. But maybe ... it is easier to perform operations on a set of tokens as opposed to singletons. That's why `whatlies` has this notion of a `TokenSet`. We will repeat the steps we took before but now on a tokenset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whatlies import EmbeddingSet\n",
    "t = EmbeddingSet(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can fetch a token from a tokenset and do operations on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['man'] | (t['man'] - t['woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the idea of a `TokenSet` is that you can also do these operations in bulk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t | (t['man'] - t['woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes plotting just a bit lot easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ts = t | (t['king'] - t['queen'])\n",
    "x_axis = t['man'] | (t['king'] - t['queen'])\n",
    "y_axis = t['woman'] | (t['king'] - t['queen'])\n",
    "(new_ts\n",
    "  .plot(kind='scatter', x_axis=x_axis, y_axis=y_axis)\n",
    "  .plot(kind='text', x_axis=x_axis, y_axis=y_axis));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easier to reason about embeddings when they are sets as opposed to seperate singletons. We've still only covered a small portion of embeddings that we have support for though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
